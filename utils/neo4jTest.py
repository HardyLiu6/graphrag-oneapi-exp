#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Neo4j å›¾æ•°æ®åº“å¯¼å…¥è„šæœ¬
====================

åŠŸèƒ½æè¿°:
    å°† GraphRAG ç”Ÿæˆçš„ Parquet æ–‡ä»¶æ•°æ®å¯¼å…¥åˆ° Neo4j å›¾æ•°æ®åº“ä¸­ï¼Œ
    æ„å»ºåŒ…å«æ–‡æ¡£ã€æ–‡æœ¬å—ã€å®ä½“ã€å…³ç³»ã€ç¤¾åŒºã€åå˜é‡çš„çŸ¥è¯†å›¾è°±ã€‚

å›¾è°±ç»“æ„:
    èŠ‚ç‚¹ç±»å‹:
        - __Document__  : åŸå§‹æ–‡æ¡£
        - __Chunk__     : æ–‡æœ¬å—ï¼ˆæ–‡æ¡£åˆ‡åˆ†åçš„ç‰‡æ®µï¼‰
        - __Entity__    : å®ä½“ï¼ˆå¯é™„åŠ ç±»å‹æ ‡ç­¾ï¼Œå¦‚ Personã€Location ç­‰ï¼‰
        - __Community__ : ç¤¾åŒºï¼ˆå®ä½“èšç±»ï¼‰
        - __Covariate__ : åå˜é‡ï¼ˆå£°æ˜/äº‹ä»¶ç­‰é™„åŠ ä¿¡æ¯ï¼‰
        - Finding       : ç¤¾åŒºå‘ç°ï¼ˆç¤¾åŒºæŠ¥å‘Šä¸­çš„å…³é”®å‘ç°ï¼‰

    å…³ç³»ç±»å‹:
        - PART_OF       : Chunk -> Documentï¼ˆæ–‡æœ¬å—å±äºæ–‡æ¡£ï¼‰
        - HAS_ENTITY    : Chunk -> Entityï¼ˆæ–‡æœ¬å—åŒ…å«å®ä½“ï¼‰
        - RELATED       : Entity -> Entityï¼ˆå®ä½“é—´å…³ç³»ï¼‰
        - IN_COMMUNITY  : Entity -> Communityï¼ˆå®ä½“å±äºç¤¾åŒºï¼‰
        - HAS_CHUNK     : Community -> Chunkï¼ˆç¤¾åŒºå…³è”æ–‡æœ¬å—ï¼‰
        - HAS_FINDING   : Community -> Findingï¼ˆç¤¾åŒºåŒ…å«å‘ç°ï¼‰
        - HAS_COVARIATE : Chunk -> Covariateï¼ˆæ–‡æœ¬å—å…³è”åå˜é‡ï¼‰

Neo4j å¸¸ç”¨æŸ¥è¯¢ç¤ºä¾‹:
    1. æŸ¥çœ‹å®ä½“å…³ç³»å›¾:
        MATCH path = (:__Entity__)-[:RELATED]->(:__Entity__)
        RETURN path LIMIT 200

    2. æŸ¥çœ‹æ–‡æ¡£ä¸æ–‡æœ¬å—:
        MATCH (d:__Document__) WITH d LIMIT 1
        MATCH path = (d)<-[:PART_OF]-(c:__Chunk__)
        RETURN path LIMIT 100

    3. æŸ¥çœ‹ç¤¾åŒºä¸å®ä½“:
        MATCH (c:__Community__) WITH c LIMIT 1
        MATCH path = (c)<-[:IN_COMMUNITY]-()-[:RELATED]-(:__Entity__)
        RETURN path LIMIT 100

    4. æ¸…ç©ºæ•°æ®åº“:
        MATCH (n) CALL { WITH n DETACH DELETE n } IN TRANSACTIONS OF 25000 ROWS;

ä¾èµ–å®‰è£…:
    pip install pandas neo4j-rust-ext

ä½œè€…: LiuJunDa
æ—¥æœŸ: 2026-01-27
æ›´æ–°: 2026-02-04
"""

import os
import sys
import time
import logging
import argparse
from pathlib import Path
from typing import Optional, cast, LiteralString

import pandas as pd
from neo4j import GraphDatabase
from neo4j.exceptions import ServiceUnavailable, AuthError


# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================================
# é»˜è®¤é…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
# ============================================================================
DEFAULT_CONFIG = {
    "GRAPHRAG_FOLDER": os.getenv("GRAPHRAG_FOLDER", "/home/sunlight/Projects/graphrag-oneapi-exp/output"),
    "NEO4J_URI": os.getenv("NEO4J_URI", "bolt://localhost:17687"),
    "NEO4J_USERNAME": os.getenv("NEO4J_USERNAME", "neo4j"),
    "NEO4J_PASSWORD": os.getenv("NEO4J_PASSWORD", "12345678"),
    "NEO4J_DATABASE": os.getenv("NEO4J_DATABASE", "neo4j"),
    "BATCH_SIZE": int(os.getenv("BATCH_SIZE", "1000")),
}


# ============================================================================
# Cypher è¯­å¥å®šä¹‰
# ============================================================================

# æ•°æ®åº“çº¦æŸï¼ˆç¡®ä¿æ•°æ®å”¯ä¸€æ€§å’ŒæŸ¥è¯¢æ€§èƒ½ï¼‰
CONSTRAINT_STATEMENTS = [
    "CREATE CONSTRAINT document_id IF NOT EXISTS FOR (d:__Document__) REQUIRE d.id IS UNIQUE",
    "CREATE CONSTRAINT chunk_id IF NOT EXISTS FOR (c:__Chunk__) REQUIRE c.id IS UNIQUE",
    "CREATE CONSTRAINT entity_id IF NOT EXISTS FOR (e:__Entity__) REQUIRE e.id IS UNIQUE",
    "CREATE CONSTRAINT entity_name IF NOT EXISTS FOR (e:__Entity__) REQUIRE e.name IS UNIQUE",
    "CREATE CONSTRAINT community_id IF NOT EXISTS FOR (c:__Community__) REQUIRE c.community IS UNIQUE",
    "CREATE CONSTRAINT covariate_title IF NOT EXISTS FOR (e:__Covariate__) REQUIRE e.title IS UNIQUE",
    "CREATE CONSTRAINT related_id IF NOT EXISTS FOR ()-[rel:RELATED]->() REQUIRE rel.id IS UNIQUE",
]

# å¯¼å…¥æ–‡æ¡£èŠ‚ç‚¹
DOCUMENT_STATEMENT = """
MERGE (d:__Document__ {id: value.id})
SET d += value {.title, .text}
"""

# å¯¼å…¥æ–‡æœ¬å—èŠ‚ç‚¹å¹¶å…³è”æ–‡æ¡£
CHUNK_STATEMENT = """
MERGE (c:__Chunk__ {id: value.id})
SET c += value {.text, .n_tokens}
WITH c, value
UNWIND value.document_ids AS document
MATCH (d:__Document__ {id: document})
MERGE (c)-[:PART_OF]->(d)
"""

# å¯¼å…¥å®ä½“èŠ‚ç‚¹å¹¶å…³è”æ–‡æœ¬å—
ENTITY_STATEMENT = """
MERGE (e:__Entity__ {id: value.id})
SET e += value {.title, .type, .description, .human_readable_id, .id, .text_unit_ids}
SET e.name = value.title
WITH e, value
CALL apoc.create.addLabels(e, 
    CASE WHEN coalesce(value.type, "") = "" 
    THEN [] 
    ELSE [apoc.text.upperCamelCase(replace(value.type, '"', ''))] 
    END
) YIELD node
UNWIND value.text_unit_ids AS text_unit
MATCH (c:__Chunk__ {id: text_unit})
MERGE (c)-[:HAS_ENTITY]->(e)
"""

# å¯¼å…¥å®ä½“é—´å…³ç³»
RELATIONSHIP_STATEMENT = """
MATCH (source:__Entity__ {name: replace(value.source, '"', '')})
MATCH (target:__Entity__ {name: replace(value.target, '"', '')})
MERGE (source)-[rel:RELATED {id: value.id}]->(target)
SET rel += value {.weight, .human_readable_id, .description, .text_unit_ids}
RETURN count(*) AS createdRels
"""

# å¯¼å…¥ç¤¾åŒºæŠ¥å‘ŠèŠ‚ç‚¹
COMMUNITY_REPORT_STATEMENT = """
MERGE (c:__Community__ {id: value.id})
SET c += value {.community, .level, .title, .rank, .rating_explanation, .full_content, .summary}
WITH c, value
UNWIND range(0, size(value.findings) - 1) AS finding_idx
WITH c, value, finding_idx, value.findings[finding_idx] AS finding
MERGE (c)-[:HAS_FINDING]->(f:Finding {id: finding_idx})
SET f += finding
"""

# å¯¼å…¥ç¤¾åŒºèŠ‚ç‚¹å¹¶å…³è”å®ä½“
COMMUNITY_STATEMENT = """
MERGE (c:__Community__ {community: value.id})
SET c += value {.level}
WITH *
UNWIND value.text_unit_ids AS text_unit_id
MATCH (t:__Chunk__ {id: text_unit_id})
MERGE (c)-[:HAS_CHUNK]->(t)
WITH *
UNWIND value.relationship_ids AS rel_id
MATCH (start:__Entity__)-[:RELATED {id: rel_id}]->(end:__Entity__)
MERGE (start)-[:IN_COMMUNITY]->(c)
MERGE (end)-[:IN_COMMUNITY]->(c)
RETURN count(DISTINCT c) AS createdCommunities
"""

# å¯¼å…¥åå˜é‡èŠ‚ç‚¹
COVARIATE_STATEMENT = """
MERGE (c:__Covariate__ {id: value.id})
SET c += apoc.map.clean(value, ["text_unit_id", "document_ids", "n_tokens"], [NULL, ""])
WITH c, value
MATCH (ch:__Chunk__ {id: value.text_unit_id})
MERGE (ch)-[:HAS_COVARIATE]->(c)
"""


# ============================================================================
# æ ¸å¿ƒåŠŸèƒ½ç±»
# ============================================================================

class GraphRAGImporter:
    """
    GraphRAG æ•°æ®å¯¼å…¥å™¨
    
    è´Ÿè´£å°† GraphRAG ç”Ÿæˆçš„ Parquet æ–‡ä»¶æ‰¹é‡å¯¼å…¥åˆ° Neo4j å›¾æ•°æ®åº“ï¼Œ
    æ”¯æŒæ‰¹å¤„ç†ã€é”™è¯¯å¤„ç†å’Œè¿›åº¦æ˜¾ç¤ºã€‚
    """
    
    def __init__(
        self,
        uri: str,
        username: str,
        password: str,
        database: str = "neo4j",
        batch_size: int = 1000
    ):
        """
        åˆå§‹åŒ–å¯¼å…¥å™¨
        
        å‚æ•°:
            uri: Neo4j è¿æ¥åœ°å€ï¼ˆå¦‚ bolt://localhost:7687ï¼‰
            username: æ•°æ®åº“ç”¨æˆ·å
            password: æ•°æ®åº“å¯†ç 
            database: ç›®æ ‡æ•°æ®åº“åç§°ï¼Œé»˜è®¤ä¸º "neo4j"
            batch_size: æ¯æ‰¹å¯¼å…¥çš„è®°å½•æ•°ï¼Œé»˜è®¤ä¸º 1000
        """
        self.uri = uri
        self.username = username
        self.database = database
        self.batch_size = batch_size
        self.driver = None
        
        try:
            self.driver = GraphDatabase.driver(uri, auth=(username, password))
            self.driver.verify_connectivity()
            logger.info(f"âœ“ æˆåŠŸè¿æ¥åˆ° Neo4j: {uri}")
        except ServiceUnavailable as e:
            logger.error(f"âœ— æ— æ³•è¿æ¥åˆ° Neo4j æœåŠ¡å™¨: {e}")
            raise
        except AuthError as e:
            logger.error(f"âœ— Neo4j è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç : {e}")
            raise
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.driver:
            self.driver.close()
            logger.info("Neo4j è¿æ¥å·²å…³é—­")
    
    def __enter__(self):
        """æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºæ—¶è‡ªåŠ¨å…³é—­è¿æ¥"""
        self.close()
    
    def execute_statement(self, statement: str) -> None:
        """
        æ‰§è¡Œå•æ¡ Cypher è¯­å¥
        
        å‚æ•°:
            statement: è¦æ‰§è¡Œçš„ Cypher æŸ¥è¯¢è¯­å¥
        """
        if self.driver is None:
            raise RuntimeError("æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–")
        with self.driver.session(database=self.database) as session:
            session.run(cast(LiteralString, statement))
    
    def create_constraints(self) -> None:
        """åˆ›å»ºæ•°æ®åº“çº¦æŸï¼Œç¡®ä¿èŠ‚ç‚¹å”¯ä¸€æ€§å¹¶ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½"""
        logger.info("æ­£åœ¨åˆ›å»ºæ•°æ®åº“çº¦æŸ...")
        for statement in CONSTRAINT_STATEMENTS:
            try:
                self.execute_statement(statement)
                logger.debug(f"  çº¦æŸåˆ›å»ºæˆåŠŸ: {statement[:60]}...")
            except Exception as e:
                # çº¦æŸå¯èƒ½å·²å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
                logger.debug(f"  çº¦æŸè·³è¿‡ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")
        logger.info("âœ“ æ•°æ®åº“çº¦æŸåˆ›å»ºå®Œæˆ")
    
    def batched_import(
        self,
        statement: str,
        df: pd.DataFrame,
        description: str = "æ•°æ®"
    ) -> int:
        """
        æ‰¹é‡å¯¼å…¥æ•°æ®åˆ° Neo4j
        
        å°† DataFrame åˆ†æ‰¹æ¬¡å¯¼å…¥æ•°æ®åº“ï¼Œé¿å…å•æ¬¡å¯¼å…¥æ•°æ®é‡è¿‡å¤§å¯¼è‡´å†…å­˜é—®é¢˜ã€‚
        
        å‚æ•°:
            statement: Cypher å¯¼å…¥è¯­å¥ï¼ˆä½¿ç”¨ value å˜é‡å¼•ç”¨æ¯è¡Œæ•°æ®ï¼‰
            df: è¦å¯¼å…¥çš„ Pandas DataFrame
            description: æ•°æ®æè¿°ï¼Œç”¨äºæ—¥å¿—æ˜¾ç¤º
        
        è¿”å›:
            æˆåŠŸå¯¼å…¥çš„æ€»è¡Œæ•°
        """
        total = len(df)
        if total == 0:
            logger.warning(f"  {description} æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¯¼å…¥")
            return 0
        
        start_time = time.time()
        imported = 0
        
        logger.info(f"å¼€å§‹å¯¼å…¥ {description}ï¼ˆå…± {total} æ¡ï¼‰...")
        
        if self.driver is None:
            raise RuntimeError("æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–")
        
        for start in range(0, total, self.batch_size):
            end = min(start + self.batch_size, total)
            batch = df.iloc[start:end]
            
            try:
                query_text = cast(LiteralString, "UNWIND $rows AS value " + statement)
                result = self.driver.execute_query(
                    query_text,
                    rows=batch.to_dict('records'),
                    database_=self.database
                )
                imported += len(batch)
                
                # æ˜¾ç¤ºè¿›åº¦å’Œç»Ÿè®¡
                progress = imported / total * 100
                counters = result.summary.counters
                logger.info(f"  è¿›åº¦: {imported}/{total} ({progress:.1f}%) - {counters}")
                
            except Exception as e:
                logger.error(f"  æ‰¹æ¬¡å¯¼å…¥å¤±è´¥ [{start}:{end}]: {e}")
                raise
        
        elapsed = time.time() - start_time
        logger.info(f"âœ“ {description}: {total} æ¡è®°å½•å¯¼å…¥å®Œæˆï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
        return total


def check_parquet_file(folder: str, filename: str) -> Optional[Path]:
    """
    æ£€æŸ¥ Parquet æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    
    å‚æ•°:
        folder: æ–‡ä»¶å¤¹è·¯å¾„
        filename: æ–‡ä»¶åï¼ˆå¦‚ documents.parquetï¼‰
    
    è¿”å›:
        Path å¯¹è±¡ï¼ˆæ–‡ä»¶å­˜åœ¨æ—¶ï¼‰æˆ– Noneï¼ˆæ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼‰
    """
    filepath = Path(folder) / filename
    if filepath.exists():
        return filepath
    logger.warning(f"  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {filepath}")
    return None


def import_all_data(importer: GraphRAGImporter, data_folder: str) -> dict:
    """
    æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¯¼å…¥æµç¨‹
    
    æŒ‰ç…§ä¾èµ–é¡ºåºå¯¼å…¥æ‰€æœ‰æ•°æ®ï¼š
    æ–‡æ¡£ -> æ–‡æœ¬å— -> å®ä½“ -> å…³ç³» -> ç¤¾åŒºæŠ¥å‘Š -> ç¤¾åŒº -> åå˜é‡
    
    å‚æ•°:
        importer: GraphRAGImporter å®ä¾‹
        data_folder: Parquet æ–‡ä»¶æ‰€åœ¨ç›®å½•
    
    è¿”å›:
        åŒ…å«å„ç±»æ•°æ®å¯¼å…¥æ•°é‡çš„å­—å…¸
    """
    stats = {}
    
    # æ­¥éª¤ 1: åˆ›å»ºçº¦æŸ
    importer.create_constraints()
    
    # æ­¥éª¤ 2: å¯¼å…¥æ–‡æ¡£
    logger.info("-" * 50)
    if filepath := check_parquet_file(data_folder, "documents.parquet"):
        df = pd.read_parquet(filepath, columns=["id", "title", "text"])
        stats["æ–‡æ¡£"] = importer.batched_import(DOCUMENT_STATEMENT, df, "æ–‡æ¡£")
    
    # æ­¥éª¤ 3: å¯¼å…¥æ–‡æœ¬å—
    logger.info("-" * 50)
    if filepath := check_parquet_file(data_folder, "text_units.parquet"):
        df = pd.read_parquet(filepath, columns=[
            "id", "text", "n_tokens", "document_ids", 
            "entity_ids", "relationship_ids", "covariate_ids"
        ])
        stats["æ–‡æœ¬å—"] = importer.batched_import(CHUNK_STATEMENT, df, "æ–‡æœ¬å—")
    
    # æ­¥éª¤ 4: å¯¼å…¥å®ä½“
    logger.info("-" * 50)
    if filepath := check_parquet_file(data_folder, "entities.parquet"):
        df = pd.read_parquet(filepath, columns=[
            "title", "type", "description", "human_readable_id", "id", "text_unit_ids"
        ])
        stats["å®ä½“"] = importer.batched_import(ENTITY_STATEMENT, df, "å®ä½“")
    
    # æ­¥éª¤ 5: å¯¼å…¥å…³ç³»
    logger.info("-" * 50)
    if filepath := check_parquet_file(data_folder, "relationships.parquet"):
        df = pd.read_parquet(filepath, columns=[
            "source", "target", "id", "weight", 
            "human_readable_id", "description", "text_unit_ids"
        ])
        stats["å…³ç³»"] = importer.batched_import(RELATIONSHIP_STATEMENT, df, "å…³ç³»")
    
    # æ­¥éª¤ 6: å¯¼å…¥ç¤¾åŒºæŠ¥å‘Š
    logger.info("-" * 50)
    if filepath := check_parquet_file(data_folder, "community_reports.parquet"):
        df = pd.read_parquet(filepath, columns=[
            "id", "community", "findings", "title", "summary",
            "level", "rank", "rating_explanation", "full_content"
        ])
        stats["ç¤¾åŒºæŠ¥å‘Š"] = importer.batched_import(COMMUNITY_REPORT_STATEMENT, df, "ç¤¾åŒºæŠ¥å‘Š")
    
    # æ­¥éª¤ 7: å¯¼å…¥ç¤¾åŒº
    logger.info("-" * 50)
    if filepath := check_parquet_file(data_folder, "communities.parquet"):
        df = pd.read_parquet(filepath, columns=[
            "id", "level", "title", "text_unit_ids", "relationship_ids"
        ])
        stats["ç¤¾åŒº"] = importer.batched_import(COMMUNITY_STATEMENT, df, "ç¤¾åŒº")
    
    # æ­¥éª¤ 8: å¯¼å…¥åå˜é‡
    logger.info("-" * 50)
    if filepath := check_parquet_file(data_folder, "covariates.parquet"):
        df = pd.read_parquet(filepath)
        stats["åå˜é‡"] = importer.batched_import(COVARIATE_STATEMENT, df, "åå˜é‡")
    
    return stats


def print_summary(stats: dict) -> None:
    """
    æ‰“å°å¯¼å…¥æ‘˜è¦
    
    å‚æ•°:
        stats: åŒ…å«å„ç±»æ•°æ®å¯¼å…¥æ•°é‡çš„å­—å…¸
    """
    logger.info("=" * 50)
    logger.info("ğŸ“Š å¯¼å…¥å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
    logger.info("=" * 50)
    total = 0
    for name, count in stats.items():
        logger.info(f"  â€¢ {name}: {count} æ¡")
        total += count
    logger.info("-" * 50)
    logger.info(f"  æ€»è®¡: {total} æ¡è®°å½•")
    logger.info("=" * 50)


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    è¿”å›:
        argparse.Namespace å¯¹è±¡
    """
    parser = argparse.ArgumentParser(
        description="GraphRAG æ•°æ®å¯¼å…¥ Neo4j å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
    # ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œ
    python neo4jTest.py

    # æŒ‡å®šæ•°æ®ç›®å½•
    python neo4jTest.py --folder /path/to/output

    # æŒ‡å®š Neo4j è¿æ¥å‚æ•°
    python neo4jTest.py --uri bolt://localhost:7687 --password mypassword

    # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
    python neo4jTest.py -v

ç¯å¢ƒå˜é‡:
    GRAPHRAG_FOLDER  - Parquet æ–‡ä»¶ç›®å½•
    NEO4J_URI        - Neo4j è¿æ¥åœ°å€
    NEO4J_USERNAME   - Neo4j ç”¨æˆ·å
    NEO4J_PASSWORD   - Neo4j å¯†ç 
    NEO4J_DATABASE   - Neo4j æ•°æ®åº“å
    BATCH_SIZE       - æ‰¹å¤„ç†å¤§å°
        """
    )
    parser.add_argument(
        "--folder", "-f",
        default=DEFAULT_CONFIG["GRAPHRAG_FOLDER"],
        help=f"Parquet æ–‡ä»¶ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: {DEFAULT_CONFIG['GRAPHRAG_FOLDER']}ï¼‰"
    )
    parser.add_argument(
        "--uri", "-u",
        default=DEFAULT_CONFIG["NEO4J_URI"],
        help=f"Neo4j è¿æ¥ URIï¼ˆé»˜è®¤: {DEFAULT_CONFIG['NEO4J_URI']}ï¼‰"
    )
    parser.add_argument(
        "--username",
        default=DEFAULT_CONFIG["NEO4J_USERNAME"],
        help=f"Neo4j ç”¨æˆ·åï¼ˆé»˜è®¤: {DEFAULT_CONFIG['NEO4J_USERNAME']}ï¼‰"
    )
    parser.add_argument(
        "--password", "-p",
        default=DEFAULT_CONFIG["NEO4J_PASSWORD"],
        help="Neo4j å¯†ç "
    )
    parser.add_argument(
        "--database", "-d",
        default=DEFAULT_CONFIG["NEO4J_DATABASE"],
        help=f"Neo4j æ•°æ®åº“åç§°ï¼ˆé»˜è®¤: {DEFAULT_CONFIG['NEO4J_DATABASE']}ï¼‰"
    )
    parser.add_argument(
        "--batch-size", "-b",
        type=int,
        default=DEFAULT_CONFIG["BATCH_SIZE"],
        help=f"æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤: {DEFAULT_CONFIG['BATCH_SIZE']}ï¼‰"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—ï¼ˆDEBUG çº§åˆ«ï¼‰"
    )
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°å…¥å£"""
    args = parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not Path(args.folder).exists():
        logger.error(f"âœ— æ•°æ®ç›®å½•ä¸å­˜åœ¨: {args.folder}")
        sys.exit(1)
    
    logger.info("=" * 50)
    logger.info("ğŸš€ GraphRAG -> Neo4j æ•°æ®å¯¼å…¥å·¥å…·")
    logger.info("=" * 50)
    logger.info(f"æ•°æ®ç›®å½•: {args.folder}")
    logger.info(f"Neo4j åœ°å€: {args.uri}")
    logger.info(f"ç›®æ ‡æ•°æ®åº“: {args.database}")
    logger.info(f"æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    logger.info("=" * 50)
    
    try:
        # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿è¿æ¥æ­£ç¡®å…³é—­
        with GraphRAGImporter(
            uri=args.uri,
            username=args.username,
            password=args.password,
            database=args.database,
            batch_size=args.batch_size
        ) as importer:
            stats = import_all_data(importer, args.folder)
            print_summary(stats)
            
    except KeyboardInterrupt:
        logger.warning("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(130)
    except Exception as e:
        logger.error(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        sys.exit(1)
    
    logger.info("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")


if __name__ == "__main__":
    main()










